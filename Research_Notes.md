## Reasearch
* Web Crawling refers to downloading and storing the contents of a large number of websites


# Achievements
* Being able to gather all the links in a website
* Use deque and stack to gather the links recursively, more like a DFS approach
* Write the output to a file
* Setup Unit tests
* Added threading to some parts of the application
* Write unit tests for utils

# Aim
* Look at the things I need to work on


# Things I need to work on
* Make sure the threading and optimization are efficient
* Write unit tests for Crawler class
* Check error hanndling again
* Is it possible to get the data even thought there is a connection disrupt?
