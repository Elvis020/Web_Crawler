## Reasearch
* Web Crawling refers to downloading and storing the contents of a large number of websites


# Achievements
* Being able to gather all the links in a website
* Use deque and stack to gather the links recursively, more like a DFS approach
* Write the output to a file

# Aim
* Look at the things I need to work on


# Things I need to work on
* Some websites take time to finish, need to add threading and optimization
* Add comments to the functions
* Setup Unit tests
